{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network\n",
    "\n",
    "\n",
    "In this notebook, we are going to create and train a simple neural network on the digits dataset using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the data and make them into pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# Normalize\n",
    "\n",
    "X -= X.mean(axis=0)\n",
    "X /= np.std(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "f, axes = plt.subplots(1, 3)\n",
    "for i, axe in enumerate(axes):\n",
    "    axe.imshow(X[i].reshape(8, 8))\n",
    "\n",
    "x = torch.tensor(X_train).float()\n",
    "y = torch.tensor(y_train).long()\n",
    "n, p = x.shape\n",
    "x_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network\n",
    "\n",
    "We will work with a simple network with two layers (one hidden layer).\n",
    "\n",
    "The input $x$ is transformed into the output $z$ by the following operations:\n",
    "\n",
    "$$y = \\tanh(W_1x + b_1)$$\n",
    "$$z = W_2y + b_2$$\n",
    "\n",
    "**Exercise 1**: Define a function `net(x, W1, b1, W2, b2)` that implements this transform. Remember that `x` is a matrix of size $n\\times p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(x, W1, b1, W2, b2):\n",
    "    y = x.mm(W1) + b1\n",
    "    y = torch.tanh(y)\n",
    "    z = y.mm(W2) + b2\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us specify the parameters of the network, `W1, b1, W2, b2`. You can chose the size of the hidden layer, but the input and output sizes are determined by the problem.\n",
    "\n",
    "**Exercise 2**: Define a set of parameters `W1, b1, W2, b2`, where you chose the size of the hidden layer. Make sure that all these parameters have their `requires_grad` flag set to true, so that we can compute the gradient with respect to them.\n",
    "\n",
    "In order to check that eveything works, compute `net(x, W1, b1, W2, b2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 20\n",
    "input_size = 64\n",
    "output_size = 10\n",
    "\n",
    "W1 = torch.randn(input_size, hidden_size, requires_grad=True)\n",
    "b1 = torch.randn(hidden_size, requires_grad=True)\n",
    "W2 = torch.randn(hidden_size, output_size, requires_grad=True)\n",
    "b2 = torch.randn(output_size, requires_grad=True)\n",
    "output = net(x, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a cost function. We will use the classical cross entropy loss. It is imported from pytorch in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Compute the current loss of the network, and then back-propagate to compute the gradient with respect to the parameters. Check the gradient with respect to W1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.232069492340088\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0024, -0.0057, -0.0009,  ..., -0.0020,  0.0006,  0.0025],\n",
      "        [-0.0165, -0.0383, -0.0030,  ..., -0.0151,  0.0094,  0.0178],\n",
      "        ...,\n",
      "        [ 0.0186,  0.0295, -0.0262,  ...,  0.0454, -0.0241, -0.0166],\n",
      "        [ 0.0064,  0.0192, -0.0248,  ...,  0.0027, -0.0168,  0.0091],\n",
      "        [ 0.0021,  0.0032, -0.0005,  ..., -0.0016, -0.0009,  0.0075]])\n"
     ]
    }
   ],
   "source": [
    "loss = cross_entropy(output, y)\n",
    "print(loss.item())\n",
    "loss.backward()\n",
    "print(W1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to train our network!\n",
    "\n",
    "But first, we will need to compute the accuracy of the network, on the train and test set.\n",
    "\n",
    "**Exercise 4**: Define a function `accuracy(X, y, W1, b1, W2, b2)` that computes the accuracy of the network on the dataset `x`with true labels `y`. Remember that the predicted class at the output of the network is computed as the argmaximum of the output. Compute the current accuracy of the network on the train set. Is it normal ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10393466963622866"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(X, y, W1, b1, W2, b2):\n",
    "    output = net(X, W1, b1, W2, b2)\n",
    "    pred = output.argmax(axis=1)\n",
    "    return (pred == y).sum().item() / len(y)\n",
    "\n",
    "\n",
    "accuracy(x, y, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We are now ready to train the network, using back-propagation and stochastic gradient descent.\n",
    "First, we define the number of iterations of the algorithm, the step size, and the batch size. We also reinitialize the weights. Finally, we will store the train and test accuracy during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "step_size = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "W1 = torch.randn(input_size, hidden_size, requires_grad=True)\n",
    "b1 = torch.randn(hidden_size, requires_grad=True)\n",
    "W2 = torch.randn(hidden_size, output_size, requires_grad=True)\n",
    "b2 = torch.randn(output_size, requires_grad=True)\n",
    "\n",
    "test_list = []\n",
    "train_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Complete the following training list, so that each parameter is updated at each iteration.\n",
    "\n",
    "Remember that at each iteration, you should:\n",
    "* compute the output of the network with respect to the batch\n",
    "* Compute the loss, and backpropagate\n",
    "* Update each parameter with gradient descent\n",
    "* Refresh the gradient of each parameter. To do so, you can do:\n",
    "\n",
    "```\n",
    "W1 = W1.detach()\n",
    "W1.requires_grad=True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Train loss: 7.466 Train acc: 0.061 Test acc 0.084\n",
      "Iteration 10 Train loss: 6.943 Train acc: 0.103 Test acc 0.138\n",
      "Iteration 20 Train loss: 4.972 Train acc: 0.159 Test acc 0.209\n",
      "Iteration 30 Train loss: 4.123 Train acc: 0.258 Test acc 0.291\n",
      "Iteration 40 Train loss: 3.044 Train acc: 0.344 Test acc 0.356\n",
      "Iteration 50 Train loss: 2.067 Train acc: 0.425 Test acc 0.436\n",
      "Iteration 60 Train loss: 2.006 Train acc: 0.509 Test acc 0.498\n",
      "Iteration 70 Train loss: 1.492 Train acc: 0.549 Test acc 0.553\n",
      "Iteration 80 Train loss: 1.596 Train acc: 0.590 Test acc 0.578\n",
      "Iteration 90 Train loss: 1.040 Train acc: 0.615 Test acc 0.620\n",
      "Iteration 100 Train loss: 1.625 Train acc: 0.642 Test acc 0.638\n",
      "Iteration 110 Train loss: 1.150 Train acc: 0.656 Test acc 0.647\n",
      "Iteration 120 Train loss: 0.956 Train acc: 0.676 Test acc 0.667\n",
      "Iteration 130 Train loss: 0.923 Train acc: 0.703 Test acc 0.691\n",
      "Iteration 140 Train loss: 1.431 Train acc: 0.722 Test acc 0.716\n",
      "Iteration 150 Train loss: 1.006 Train acc: 0.736 Test acc 0.722\n",
      "Iteration 160 Train loss: 0.838 Train acc: 0.754 Test acc 0.722\n",
      "Iteration 170 Train loss: 0.547 Train acc: 0.766 Test acc 0.729\n",
      "Iteration 180 Train loss: 0.880 Train acc: 0.777 Test acc 0.738\n",
      "Iteration 190 Train loss: 0.566 Train acc: 0.788 Test acc 0.751\n",
      "Iteration 200 Train loss: 0.698 Train acc: 0.796 Test acc 0.764\n",
      "Iteration 210 Train loss: 0.464 Train acc: 0.807 Test acc 0.773\n",
      "Iteration 220 Train loss: 0.849 Train acc: 0.811 Test acc 0.778\n",
      "Iteration 230 Train loss: 0.814 Train acc: 0.817 Test acc 0.778\n",
      "Iteration 240 Train loss: 0.612 Train acc: 0.829 Test acc 0.782\n",
      "Iteration 250 Train loss: 0.413 Train acc: 0.832 Test acc 0.789\n",
      "Iteration 260 Train loss: 0.748 Train acc: 0.837 Test acc 0.793\n",
      "Iteration 270 Train loss: 0.793 Train acc: 0.840 Test acc 0.800\n",
      "Iteration 280 Train loss: 0.375 Train acc: 0.847 Test acc 0.796\n",
      "Iteration 290 Train loss: 0.613 Train acc: 0.853 Test acc 0.793\n",
      "Iteration 300 Train loss: 0.261 Train acc: 0.854 Test acc 0.802\n",
      "Iteration 310 Train loss: 0.337 Train acc: 0.855 Test acc 0.802\n",
      "Iteration 320 Train loss: 0.420 Train acc: 0.860 Test acc 0.807\n",
      "Iteration 330 Train loss: 0.419 Train acc: 0.864 Test acc 0.807\n",
      "Iteration 340 Train loss: 0.204 Train acc: 0.868 Test acc 0.807\n",
      "Iteration 350 Train loss: 0.327 Train acc: 0.873 Test acc 0.809\n",
      "Iteration 360 Train loss: 0.256 Train acc: 0.878 Test acc 0.809\n",
      "Iteration 370 Train loss: 0.286 Train acc: 0.880 Test acc 0.811\n",
      "Iteration 380 Train loss: 0.455 Train acc: 0.883 Test acc 0.816\n",
      "Iteration 390 Train loss: 0.297 Train acc: 0.882 Test acc 0.818\n",
      "Iteration 400 Train loss: 0.595 Train acc: 0.888 Test acc 0.816\n",
      "Iteration 410 Train loss: 0.294 Train acc: 0.890 Test acc 0.818\n",
      "Iteration 420 Train loss: 0.237 Train acc: 0.892 Test acc 0.818\n",
      "Iteration 430 Train loss: 0.282 Train acc: 0.892 Test acc 0.820\n",
      "Iteration 440 Train loss: 0.344 Train acc: 0.894 Test acc 0.827\n",
      "Iteration 450 Train loss: 0.458 Train acc: 0.898 Test acc 0.822\n",
      "Iteration 460 Train loss: 0.295 Train acc: 0.901 Test acc 0.822\n",
      "Iteration 470 Train loss: 0.380 Train acc: 0.903 Test acc 0.827\n",
      "Iteration 480 Train loss: 0.321 Train acc: 0.902 Test acc 0.831\n",
      "Iteration 490 Train loss: 0.334 Train acc: 0.906 Test acc 0.833\n",
      "Iteration 500 Train loss: 0.573 Train acc: 0.906 Test acc 0.836\n",
      "Iteration 510 Train loss: 0.411 Train acc: 0.909 Test acc 0.838\n",
      "Iteration 520 Train loss: 0.471 Train acc: 0.906 Test acc 0.833\n",
      "Iteration 530 Train loss: 0.260 Train acc: 0.908 Test acc 0.836\n",
      "Iteration 540 Train loss: 0.483 Train acc: 0.909 Test acc 0.842\n",
      "Iteration 550 Train loss: 0.384 Train acc: 0.909 Test acc 0.842\n",
      "Iteration 560 Train loss: 0.346 Train acc: 0.912 Test acc 0.844\n",
      "Iteration 570 Train loss: 0.426 Train acc: 0.912 Test acc 0.844\n",
      "Iteration 580 Train loss: 0.232 Train acc: 0.915 Test acc 0.847\n",
      "Iteration 590 Train loss: 0.164 Train acc: 0.918 Test acc 0.842\n",
      "Iteration 600 Train loss: 0.260 Train acc: 0.921 Test acc 0.842\n",
      "Iteration 610 Train loss: 0.190 Train acc: 0.922 Test acc 0.844\n",
      "Iteration 620 Train loss: 0.337 Train acc: 0.923 Test acc 0.847\n",
      "Iteration 630 Train loss: 0.349 Train acc: 0.922 Test acc 0.847\n",
      "Iteration 640 Train loss: 0.240 Train acc: 0.925 Test acc 0.847\n",
      "Iteration 650 Train loss: 0.427 Train acc: 0.927 Test acc 0.849\n",
      "Iteration 660 Train loss: 0.185 Train acc: 0.927 Test acc 0.849\n",
      "Iteration 670 Train loss: 0.088 Train acc: 0.927 Test acc 0.849\n",
      "Iteration 680 Train loss: 0.374 Train acc: 0.929 Test acc 0.849\n",
      "Iteration 690 Train loss: 0.270 Train acc: 0.929 Test acc 0.853\n",
      "Iteration 700 Train loss: 0.132 Train acc: 0.926 Test acc 0.853\n",
      "Iteration 710 Train loss: 0.427 Train acc: 0.927 Test acc 0.853\n",
      "Iteration 720 Train loss: 0.162 Train acc: 0.932 Test acc 0.851\n",
      "Iteration 730 Train loss: 0.307 Train acc: 0.932 Test acc 0.856\n",
      "Iteration 740 Train loss: 0.383 Train acc: 0.932 Test acc 0.860\n",
      "Iteration 750 Train loss: 0.273 Train acc: 0.932 Test acc 0.860\n",
      "Iteration 760 Train loss: 0.230 Train acc: 0.929 Test acc 0.862\n",
      "Iteration 770 Train loss: 0.251 Train acc: 0.930 Test acc 0.862\n",
      "Iteration 780 Train loss: 0.247 Train acc: 0.933 Test acc 0.862\n",
      "Iteration 790 Train loss: 0.351 Train acc: 0.934 Test acc 0.862\n",
      "Iteration 800 Train loss: 0.235 Train acc: 0.935 Test acc 0.862\n",
      "Iteration 810 Train loss: 0.203 Train acc: 0.937 Test acc 0.860\n",
      "Iteration 820 Train loss: 0.111 Train acc: 0.937 Test acc 0.862\n",
      "Iteration 830 Train loss: 0.159 Train acc: 0.937 Test acc 0.864\n",
      "Iteration 840 Train loss: 0.187 Train acc: 0.938 Test acc 0.864\n",
      "Iteration 850 Train loss: 0.174 Train acc: 0.939 Test acc 0.864\n",
      "Iteration 860 Train loss: 0.280 Train acc: 0.941 Test acc 0.869\n",
      "Iteration 870 Train loss: 0.242 Train acc: 0.941 Test acc 0.869\n",
      "Iteration 880 Train loss: 0.190 Train acc: 0.943 Test acc 0.869\n",
      "Iteration 890 Train loss: 0.122 Train acc: 0.944 Test acc 0.869\n",
      "Iteration 900 Train loss: 0.134 Train acc: 0.944 Test acc 0.869\n",
      "Iteration 910 Train loss: 0.213 Train acc: 0.944 Test acc 0.869\n",
      "Iteration 920 Train loss: 0.151 Train acc: 0.944 Test acc 0.867\n",
      "Iteration 930 Train loss: 0.183 Train acc: 0.945 Test acc 0.867\n",
      "Iteration 940 Train loss: 0.177 Train acc: 0.944 Test acc 0.867\n",
      "Iteration 950 Train loss: 0.168 Train acc: 0.945 Test acc 0.867\n",
      "Iteration 960 Train loss: 0.179 Train acc: 0.944 Test acc 0.867\n",
      "Iteration 970 Train loss: 0.194 Train acc: 0.946 Test acc 0.867\n",
      "Iteration 980 Train loss: 0.185 Train acc: 0.946 Test acc 0.869\n",
      "Iteration 990 Train loss: 0.069 Train acc: 0.947 Test acc 0.869\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_iter):\n",
    "    batch_idx = torch.argsort(torch.randn(n))[:batch_size]\n",
    "    x_batch = x[batch_idx]\n",
    "    y_batch = y[batch_idx]\n",
    "    output = net(x_batch, W1, b1, W2, b2)\n",
    "    loss = cross_entropy(output, y_batch)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W1 = W1 - step_size * W1.grad\n",
    "        W1 = W1.detach()\n",
    "        W1.requires_grad = True\n",
    "        b1 = b1 - step_size * b1.grad\n",
    "        b1 = b1.detach()\n",
    "        b1.requires_grad = True\n",
    "        W2 = W2 - step_size * W2.grad\n",
    "        W2 = W2.detach()\n",
    "        W2.requires_grad = True\n",
    "        b2 = b2 - step_size * b2.grad\n",
    "        b2 = b2.detach()\n",
    "        b2.requires_grad = True\n",
    "    if i % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            train_acc = accuracy(x, y, W1, b1, W2, b2)\n",
    "            test_acc = accuracy(x_test, y_test, W1, b1, W2, b2)\n",
    "        test_list.append(test_acc)\n",
    "        train_list.append(train_acc)\n",
    "        print('Iteration {} Train loss: {:1.3f} Train acc: {:1.3f} Test acc {:1.3f}'.format(i, loss.item(), train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**: Display the learning curves. You can then play with the network and training parameters:\n",
    "what happens when you change the learning rate, the number of hidden sizes, etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcd38066510>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_list, label='test')\n",
    "plt.plot(train_list, label='train')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
